> ***What is ChatGPT?***

To answer this question, it's essential to first define a few key ideas related to Artificial Intelligence (AI).

***Artificial Intelligence*** is a subset of Computer Science that focuses on replicating human intelligence in machines. This is achieved through an engineered system that is designed to learn from various inputs and make decisions based on their interpretation of these inputs.

_**Machine Learning**_, a branch of AI, uses algorithms to learn from data and make predictions based on that data.

_**Natural Language Processing (NLP)**_ is another branch of AI that focuses on the interaction between computers and human language. Many of its tasks, such as language comprehension and generation, often harness machine learning techniques to enhance their capabilities.

_**Large Language Models (LLMs)**_ are a type of generative AI that are specifically designed to process and generate human-like text to solve NLP problems. 

***ChatGPT*** is an ***LLM*** built by OpenAI that aims to understand and generate text in a conversational manner.

> ***What Does ChatGPT Do?***


ChatGPT stands as an advanced conversational interface, designed to replicate human-like dialogue experiences. It seamlessly engages users, answers inquiries, addresses follow-up questions, challenges inaccuracies, and executes diverse tasks like text composition, code generation, and even image creation. Its intuitive and approachable user interface mimics familiar messaging paradigms, making it user-friendly and adaptable to varied interactive scenarios.

While ChatGPT itself is anchored to its training data and doesn't inherently search the internet, over time, it's core capabilities have been refined and broadened through the integration of plugins. For instance, when coupled with plugins like Wolfram Alpha, ChatGPT's computational and data querying potential is significantly amplified. Additionally, it can generate responses in various languages and styles—be it formal, informal, or humorous—tailoring its interactions to the context and preferences of the user.


> ***A Brief History of Large Language Models***

- **Early Beginnings**:  
The journey into generative AI began as early as the 1960s, with Joseph Weizenbaum's chatbot ELIZA serving as an early exemplar of Natural Language Processing (NLP).

- **Challenges and ANNs**:  
Though the first Artificial Neural Networks (ANNs) emerged in the 1940s, they were stymied by hurdles such as computational limitations and gaps in comprehending the brain's biological intricacies.

- **Backpropagation and Training**:  
The 1980s heralded a significant shift with the introduction of the backpropagation algorithm. Prior to its advent, training Neural Networks was an intricate endeavor. However, backpropagation revolutionized this, streamlining the training process by efficiently calculating the gradient of the error in relation to the neuron-associated parameters.

- **Emergence of VAEs**:  
2013 marked another pivotal moment when Kingma and Welling showcased Variational Autoencoders (VAEs) in their work, "Auto-Encoding Variational Bayes". These generative models, rooted in variational inference, innovatively employed a probabilistic interpretation of latent space.

- **Introduction of GANs**:  
A year thereafter, Ian Goodfellow presented Generative Adversarial Networks (GANs). These networks operate on a principle where a generator aims to produce data deceptive enough to be deemed real by a discriminator, which simultaneously endeavors to differentiate between genuine and counterfeit data.

- **Transformers and the Advent of LLMs**:  
A landmark achievement came in 2017 with the unveiling of the Transformer architecture by Google's researchers. These Transformers laid the groundwork for monumental language models like Bidirectional Encoder Representations from Transformers (BERT) and subsequently the Generative Pre-trained Transformers (GPT) series by OpenAI, with GPT-3 forming the foundation for ChatGPT.

- **Generative AI in the Limelight**:  
By 2022, generative AI had captured mainstream attention, democratizing powerful AI models for the masses. This shift allowed individuals and entities alike to tap into the potential of generative AI without necessitating expertise as a data scientist or machine learning engineer.



ChatGPT's training follows a two-step process: pre-training and fine-tuning.